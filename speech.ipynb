{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and Restoring of Mixed and Noise Polluted Audio Samples Using Different Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inp_folder = 'audio'\n",
    "outp_folder = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['speech.wav', 'street-noise.wav', 'music.wav', 'white-noise.wav', 'tone_10kHz.wav']\n",
    "\n",
    "audio_speech, sr_speech = sf.read(os.path.join(inp_folder, file_names[0])) # , always_2d=True\n",
    "audio_street, sr_street = sf.read(os.path.join(inp_folder, file_names[1]))\n",
    "audio_music, sr_music = sf.read(os.path.join(inp_folder, file_names[2]))\n",
    "audio_wnoise, sr_wnoise = sf.read(os.path.join(inp_folder, file_names[3]))\n",
    "audio_test_tone, sr_test_tone = sf.read(os.path.join(inp_folder, file_names[4]))\n",
    "print(\"Array Type is: {}\".format(audio_music.dtype))\n",
    "# Attenuate white noise amplitude \n",
    "attenuation_ratio = 1/4\n",
    "audio_files = [audio_speech, audio_street, audio_music, audio_wnoise * attenuation_ratio, audio_test_tone]\n",
    "sample_rates = [sr_speech, sr_street, sr_music, sr_wnoise, sr_test_tone]\n",
    "\n",
    "for name, sr, audio in zip(file_names, sample_rates, audio_files):\n",
    "    print(\"File: {}, Sample Rate: {}, Samples: {}, Time: {}sec\".format(name, sr, len(audio), len(audio)/sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_mono = True\n",
    "# Reduce from stereo to mono\n",
    "if make_mono:\n",
    "    for ii, audio in enumerate(audio_files):\n",
    "        if len(audio.shape) == 2:\n",
    "            peak_l = max(audio[:, 0])\n",
    "            peak_r = max(audio[:, 1])\n",
    "            peak_pre = max(peak_l, peak_r)\n",
    "            audio_files[ii] = np.sum(audio, axis=1)\n",
    "            audio_files[ii] /= max(audio_files[ii])\n",
    "            audio_files[ii] *= peak_pre\n",
    "            print(\"Shape File {}: {}\".format(ii+1, audio_files[ii].shape))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import samplerate\n",
    "\n",
    "max_sr = max(sample_rates)\n",
    "converter = 'sinc_best'\n",
    "\n",
    "for ii, (audio, sr) in enumerate(zip(audio_files, sample_rates)):\n",
    "    if sr != max_sr:\n",
    "        ratio = max_sr / sr\n",
    "        audio_files[ii] = samplerate.resample(audio, ratio, converter)\n",
    "        print(\"New Shape: {}\".format(audio_files[ii].shape))\n",
    "        sample_rates[ii] = max_sr\n",
    "        \n",
    "for name, audio in zip(file_names, audio_files):\n",
    "    print(\"File: {}, Sample Rate: {}, Samples: {}, Time: {}sec\".format(name, max_sr, len(audio), len(audio)/sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimal audio length\n",
    "audio_length = []\n",
    "for ii, audio in enumerate(audio_files):\n",
    "    audio_length.append(len(audio))\n",
    "max_length = min(audio_length)\n",
    "print(\"Minimal number of audio samples: {}\".format(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Audio length\n",
    "# Neede samples are samples = time * sample_rate\n",
    "time = max_length / sample_rates[audio_length.index(max_length)]\n",
    "\n",
    "adj_audio_length = []\n",
    "for sr in sample_rates:\n",
    "    adj_audio_length.append(int(time * sr))\n",
    "\n",
    "print(\"Max audio length is {} s\".format(time))\n",
    "for name, smp_has, smp_should in zip(file_names, audio_length, adj_audio_length):\n",
    "    print(\"File {} has {} Samples and must have {}\".format(name, smp_has, smp_should))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify audio file length\n",
    "for ii, smp in enumerate(adj_audio_length):\n",
    "    audio_files[ii] = audio_files[ii][:smp]\n",
    "    print(\"Shape file {}: {}\".format(ii+1, audio_files[ii].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Domain\n",
    "\n",
    "Time plot of original audio files $u$ and noise sources $v$.\n",
    "\n",
    "| Audio | Symbol |\n",
    "| --- | --- |\n",
    "| Speech | $u_s$ | \n",
    "| Street Noise | $v_{sn}$ |\n",
    "| Music | $u_m$ |\n",
    "| White Noise | $v_{wn}$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(audio_files), 1, figsize=[18, 10], sharex=False, constrained_layout = True)\n",
    "audio_names = ['Speech $u_s$', 'Street Noise $v_{sn}$', 'Music $u_m$', 'White Noise $v_{wn}$', 'Tone 10kHz']\n",
    "colors = ['blue', 'red', 'green', 'black', 'orange']\n",
    "for ii, (audio, name, sr, color) in enumerate(zip(audio_files, audio_names, sample_rates, colors)):\n",
    "    x_axis = np.arange(0, len(audio)) / sr\n",
    "    axs[ii].plot(x_axis, audio, lw=5, color=color)\n",
    "    axs[ii].set_title(name, fontsize=25)\n",
    "\n",
    "fig.supylabel('Amplitude', fontsize=20)\n",
    "fig.supxlabel('Time in Sec', fontsize=20)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(0)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrum\n",
    "\n",
    "Frequencies in original audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Fourier Transform\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "\n",
    "spectrums = []\n",
    "fft_x = []\n",
    "for audio, sample_rate, samples in zip(audio_files, sample_rates, adj_audio_length):\n",
    "    frequencies = fft(audio)\n",
    "    T = 1/sample_rate\n",
    "    x_axis = fftfreq(samples, T)[:samples//2]\n",
    "    spectrums.append(2.0/samples * np.abs(frequencies[0:samples//2]))\n",
    "    fft_x.append(x_axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(audio_files), 1, figsize=[18, 10], sharex=False, constrained_layout = True)\n",
    "\n",
    "for ii, (spectrum, x_axis, name, color) in enumerate(zip(spectrums, fft_x, audio_names, colors)):\n",
    "    axs[ii].plot(x_axis / 1e3, spectrum, lw=5, color=color)\n",
    "    axs[ii].set_title(name, fontsize=25)\n",
    "\n",
    "fig.supylabel('Magnitude', fontsize=20)\n",
    "fig.supxlabel('Frequency in kHz', fontsize=20)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(0)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Files\n",
    "names_source_out = ['speech_source.wav', 'street_source.wav', 'music_source.wav', 'white_noise_source.wav', 'tone_10kHz_source.wav']\n",
    "for name, audio, sr in zip(names_source_out, audio_files, sample_rates):\n",
    "    sf.write(os.path.join(outp_folder, name), audio, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix Signals\n",
    "\n",
    "Generate four observations $d$. \n",
    "\n",
    "1. Music + White Noise $d_{m+wn}$\n",
    "1. Street Noise + Music $d_{sn+m}$\n",
    "1. Street Noise + Music + Speech $d_{sn+m+s}$\n",
    "1. Speech + White Noise $d_{s+wn}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1           Music +       White Noise\n",
    "d_m_wn = audio_files[2] + audio_files[3]\n",
    "# 2           Street +      Music\n",
    "d_sn_m = audio_files[1] + audio_files[2]\n",
    "# 3 :         Street +      Music +             Speech\n",
    "d_sn_m_s = audio_files[1] + audio_files[2] + audio_files[0]\n",
    "# 4 :         Speech +      White Noise\n",
    "d_s_wn = audio_files[0] + audio_files[3]\n",
    "\n",
    "observations = [d_m_wn, d_sn_m, d_sn_m_s, d_s_wn]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add seed for same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Component Analysis (ICA)\n",
    "\n",
    "ICA is a method for seperating a multivariate signal into components. \n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Center $\\mathbf{d}$ by subtracting the mean\n",
    "1. Whiten\n",
    "1. Choose random initial value for the de-mixing matrix $\\mathbf{W}$\n",
    "1. Calculate the new value for $\\mathbf{W}$\n",
    "1. Normalize $\\mathbf{W}$\n",
    "1. Check whether algorithm has converged and if it hasnâ€™t, return to step 4\n",
    "1. Take the dot product of $\\mathbf{W}$ and $\\mathbf{d}$ to get the independent source signals\n",
    "$$\\mathbf{S} = \\mathbf{W}\\mathbf{d}$$\n",
    "\n",
    "Source:\n",
    "* [Independent Component Analysis (ICA) In Python, Cory Maklin](https://towardsdatascience.com/independent-component-analysis-ica-in-python-a0ef0db0955e)\n",
    "* [Separating mixed signals with Independent Component Analysis, Carsten Klein](https://towardsdatascience.com/separating-mixed-signals-with-independent-component-analysis-38205188f2f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Observation using True Signals\n",
    "\n",
    "Generating signals for observation #3 and apply to \"cocktail party problem\". \n",
    "\n",
    "One recording per existing signal is used here. The recorded signal contains a proportion of pollution caused by the other signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 :                   Street +            Music +                 Speech\n",
    "d_sn_m_s_1 = 0.3 * audio_files[1] + 0.3 * audio_files[2] + 0.4 * audio_files[0]\n",
    "d_sn_m_s_2 = 0.6 * audio_files[1] + 0.2 * audio_files[2] + 0.2 * audio_files[0]\n",
    "d_sn_m_s_3 = 0.1 * audio_files[1] + 0.7 * audio_files[2] + 0.2 * audio_files[0]\n",
    "true_signal = np.c_[audio_files[0], audio_files[1], audio_files[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=[30, 20], sharex=True, constrained_layout = True)\n",
    "fig.supylabel('Amplitude', fontsize=20)\n",
    "fig.supxlabel('Samples', fontsize=20)\n",
    "fig.suptitle('Street Noise + Music + Speech $d_{sn+m+s}$', fontsize=25)\n",
    "\n",
    "axs[0].plot(d_sn_m_s_1, lw=5, color='blue')\n",
    "axs[0].set_title('Signal 1', fontsize=25)\n",
    "axs[1].plot(d_sn_m_s_2, lw=5, color='red')\n",
    "axs[1].set_title('Signal 2', fontsize=25)\n",
    "axs[2].plot(d_sn_m_s_3, lw=5, color='black')\n",
    "axs[2].set_title('Signal 3', fontsize=25)\n",
    "\n",
    "for ax in axs: \n",
    "    ax.set_xlim(0, max_length)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and Normalize Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = np.c_[d_sn_m_s_1, d_sn_m_s_2, d_sn_m_s_3]\n",
    "\n",
    "observation /= observation.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Additional Distortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_distortion = False\n",
    "if add_distortion:\n",
    "    mixing = np.array([[0.5, 1, 0.2],\n",
    "                        [1, 0.5, 0.4],\n",
    "                        [0.5, 0.8, 1]])\n",
    "                        \n",
    "    observation = np.dot(true_signal, mixing.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ICA signal\n",
    "def save_ica_audio(audio, name):\n",
    "    names_ica = []\n",
    "    for ii in range(audio.shape[1]):\n",
    "        names_ica.append('ica_{}_ch{}.wav'.format(name, ii+1))\n",
    "\n",
    "    for ii, (name, sr) in enumerate(zip(names_ica, sample_rates)):\n",
    "        sf.write(os.path.join(outp_folder, name), audio[:,ii], sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply FastICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA() #n_components=3\n",
    "ica_recovered = ica.fit_transform(observation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_plot = False\n",
    "if simple_plot:\n",
    "    fig, axs = plt.subplots(3, 1, figsize=[30, 20], sharex=True, constrained_layout = True)\n",
    "    fig.supylabel('Amplitude', fontsize=20)\n",
    "    fig.supxlabel('Samples', fontsize=20)\n",
    "    fig.suptitle('Street Noise + Music + Speech $d_{sn+m+s}$', fontsize=25)\n",
    "    \n",
    "    axs[0].plot(ica_recovered[:, 0], lw=5, color='blue')\n",
    "    axs[0].set_title('ICA Recovered', fontsize=25)\n",
    "    axs[1].plot(ica_recovered[:, 1], lw=5, color='red')\n",
    "    axs[2].plot(ica_recovered[:, 2], lw=5, color='black')\n",
    "\n",
    "    for ax in axs: \n",
    "        ax.set_xlim(0, max_length)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.grid()\n",
    "else:\n",
    "    ica_lst = [observation, true_signal, ica_recovered]\n",
    "    ica_name = ['Observation', 'True Signal', 'ICA Recovered']\n",
    "    fig, axs = plt.subplots(3, 3, figsize=[30, 20], sharex=True, constrained_layout = True)\n",
    "    fig.supylabel('Amplitude', fontsize=20)\n",
    "    fig.supxlabel('Samples', fontsize=20)\n",
    "    fig.suptitle('Street Noise + Music + Speech $d_{sn+m+s}$', fontsize=25)\n",
    "\n",
    "    for ii, (data, name) in enumerate(zip(ica_lst, ica_name)):\n",
    "        axs[0][ii].plot(data[:, 0], lw=5, color='blue')\n",
    "        axs[0][ii].set_title(name, fontsize=25)\n",
    "        axs[1][ii].plot(data[:, 1], lw=5, color='red')\n",
    "        axs[2][ii].plot(data[:, 2], lw=5, color='black')\n",
    "\n",
    "    for row in axs: \n",
    "        for ax in row: \n",
    "            ax.set_xlim(0, max_length)\n",
    "            ax.tick_params(labelsize=15)\n",
    "            ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save observations and recovered signal\n",
    "save_ica_audio(ica_recovered, 'recv_sig3')\n",
    "save_ica_audio(observation, 'obs_sig3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform ICA for the remaining test observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ica(true_sig, observation, signal_name, add_distort=False):\n",
    "    true_sig /= true_sig.std(axis=0)\n",
    "    observation /= observation.std(axis=0)\n",
    "\n",
    "    if add_distort:\n",
    "        mixing = np.array([[0.5, 1],\n",
    "                            [0.8, 0.2]])\n",
    "        observation = np.dot(true_sig, mixing.T)\n",
    "        \n",
    "    ica = FastICA()\n",
    "    ica_recovered = ica.fit_transform(observation)\n",
    "\n",
    "    ica_lst = [observation, true_sig, ica_recovered]\n",
    "    ica_name = ['Observation', 'True Signal', 'ICA Recovered']\n",
    "    fig, axs = plt.subplots(2, 3, figsize=[30, 20], sharex=True, constrained_layout = True)\n",
    "    fig.supylabel('Amplitude', fontsize=20)\n",
    "    fig.supxlabel('Samples', fontsize=20)\n",
    "\n",
    "    save_ica_audio(ica_recovered, 'recv_{}'.format(signal_name))\n",
    "    save_ica_audio(observation, 'obs_{}'.format(signal_name))\n",
    "\n",
    "    equalize_amp = False\n",
    "    if equalize_amp:\n",
    "        ratio = np.abs(true_sig).mean() / np.abs(ica_recovered).mean()\n",
    "        for ii in range(ica_recovered.shape[1]):\n",
    "            ica_recovered[:, ii] = ica_recovered[:, ii] * ratio\n",
    "    \n",
    "    for ii, (data, name) in enumerate(zip(ica_lst, ica_name)):\n",
    "        axs[0][ii].plot(data[:, 0], lw=5, color='blue')\n",
    "        axs[0][ii].set_title(name, fontsize=25)\n",
    "        axs[1][ii].plot(data[:, 1], lw=5, color='red')\n",
    "\n",
    "    for row in axs: \n",
    "        for ax in row: \n",
    "            ax.set_xlim(0, max_length)\n",
    "            ax.tick_params(labelsize=15)\n",
    "            ax.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1                 Music +             White Noise\n",
    "d_m_wn_1 = 0.7 * audio_files[2] + 0.3 * audio_files[3]\n",
    "d_m_wn_2 = 0.3 * audio_files[2] + 0.7 * audio_files[3]\n",
    "observation_1 = np.c_[d_m_wn_1, d_m_wn_2]\n",
    "true_signal_1 = np.c_[audio_files[2], audio_files[3]]\n",
    "# 2                 Street +            Music\n",
    "d_sn_m_1 = 0.7 * audio_files[1] + 0.3 * audio_files[2]\n",
    "d_sn_m_2 = 0.3 * audio_files[1] + 0.7 * audio_files[2]\n",
    "observation_2 = np.c_[d_sn_m_1, d_sn_m_2]\n",
    "true_signal_2 = np.c_[audio_files[1], audio_files[2]]\n",
    "# 4 :               Speech +            White Noise\n",
    "d_s_wn_1 = 0.7 * audio_files[0] + 0.3 * audio_files[3]\n",
    "d_s_wn_2 = 0.3 * audio_files[0] + 0.7 * audio_files[3]\n",
    "observation_4 = np.c_[d_s_wn_1, d_s_wn_2]\n",
    "true_signal_4 = np.c_[audio_files[0], audio_files[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music + White Noise $d_{m+wn}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica(true_signal_1, observation_1, 'sig1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street Noise + Music $d_{sn+m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica(true_signal_2, observation_2, 'sig2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech + White Noise $d_{s+wn}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica(true_signal_4, observation_4, 'sig4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet Transform\n",
    "\n",
    "Source:\n",
    "* [Audio Classification using Wavelet Transform and Deep Learning, Aditya Dutt](https://medium.com/mlearning-ai/audio-classification-using-wavelet-transform-and-deep-learning-f9f0978fa246)\n",
    "* [Choose a Wavelet, MathWorks](https://de.mathworks.com/help/wavelet/gs/choose-a-wavelet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the wavelet families available\n",
    "print(pywt.families())\n",
    "# print a list of available wavelets from one family\n",
    "print(pywt.wavelist(kind='discrete'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet = 'db5'\n",
    "mode = 'symmetric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_wavelet(signal, wavelet, mode):\n",
    "    coeffs = pywt.wavedec(signal, wavelet, mode=mode)\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wavelet(coeffs, signal):\n",
    "    fig, axs = plt.subplots(len(coeffs)+1, 1, figsize=[18, 30], sharex=True, constrained_layout = True)\n",
    "    fig.supylabel('Amplitude', fontsize=20)\n",
    "\n",
    "    axs[0].plot(signal, lw=5)\n",
    "    axs[0].set_title('Mixed Data', fontsize=25)\n",
    "\n",
    "    for ii, coeff in enumerate(coeffs):\n",
    "        axs[ii+1].plot(coeff, lw=5)\n",
    "        axs[ii+1].set_title('c'+ str(ii), fontsize=25)\n",
    "\n",
    "    axs[len(coeffs)].set_xlabel('Samples', fontsize=20)\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlim(0)\n",
    "        ax.tick_params(labelsize=15)\n",
    "        ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_wavelet(coeffs, remove_st, remove_end):\n",
    "    ctn = list(range(remove_st, remove_end+1))\n",
    "    for ii in ctn:\n",
    "        coeffs[-ii] = np.zeros_like(coeffs[-ii])\n",
    "\n",
    "    return pywt.waverec(coeffs, wavelet, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech + White Noise $d_{s+wn}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_dswn = calc_wavelet(d_s_wn, wavelet, mode)\n",
    "plot_wavelet(coeff_dswn, d_s_wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recov_dswn = recover_wavelet(coeff_dswn, 0, 5)\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=[18, 10], sharex=True, constrained_layout = True)\n",
    "fig.supylabel('Amplitude', fontsize=20)\n",
    "\n",
    "axs[0].plot(d_s_wn, lw=5)\n",
    "axs[0].set_title('Mixed Data', fontsize=25)\n",
    "\n",
    "axs[1].plot(recov_dswn, lw=5)\n",
    "axs[1].set_title('Recovered Signal', fontsize=25)\n",
    "\n",
    "axs[2].plot(audio_files[0], lw=5)\n",
    "axs[2].set_title('Speech Signal', fontsize=25)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(0)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street Noise + Music $d_{sn+m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_dsnm = calc_wavelet(d_sn_m, wavelet, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wavelet(coeff_dsnm, d_sn_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recov_dsnm = recover_wavelet(coeff_dsnm, 0, 5)\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=[18, 10], sharex=True, constrained_layout = True)\n",
    "fig.supylabel('Amplitude', fontsize=20)\n",
    "\n",
    "axs[0].plot(d_sn_m, lw=5)\n",
    "axs[0].set_title('Mixed Data', fontsize=25)\n",
    "\n",
    "axs[1].plot(recov_dsnm, lw=5)\n",
    "axs[1].set_title('Recovered Signal', fontsize=25)\n",
    "\n",
    "axs[2].plot(audio_files[1], lw=5)\n",
    "axs[2].set_title('Street Noise', fontsize=25)\n",
    "\n",
    "axs[3].plot(audio_files[2], lw=5)\n",
    "axs[3].set_title('Music Signal', fontsize=25)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(0)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music + White Noise $d_{m+wn}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_dmwn = calc_wavelet(d_m_wn, wavelet, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wavelet(coeff_dmwn, d_m_wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recov_dmwn = recover_wavelet(coeff_dmwn, 0, 4)\n",
    "\n",
    "fig, axs = plt.subplots(4, 1, figsize=[18, 10], sharex=True, constrained_layout = True)\n",
    "fig.supylabel('Amplitude', fontsize=20)\n",
    "\n",
    "axs[0].plot(d_m_wn, lw=5)\n",
    "axs[0].set_title('Mixed Data', fontsize=25)\n",
    "\n",
    "axs[1].plot(recov_dmwn, lw=5)\n",
    "axs[1].set_title('Recovered Signal', fontsize=25)\n",
    "\n",
    "axs[2].plot(audio_files[3], lw=5)\n",
    "axs[2].set_title('White Noise', fontsize=25)\n",
    "\n",
    "axs[3].plot(audio_files[2], lw=5)\n",
    "axs[3].set_title('Music Signal', fontsize=25)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(0)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street Noise + Music + Speec $d_{sn+m+s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_snms = calc_wavelet(d_sn_m_s, wavelet, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wavelet(coeff_snms, d_sn_m_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recov_snms = recover_wavelet(coeff_snms, 0, 5)\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=[18, 10], sharex=True, constrained_layout = True)\n",
    "fig.supylabel('Amplitude', fontsize=20)\n",
    "\n",
    "axs[0].plot(d_sn_m_s, lw=5)\n",
    "axs[0].set_title('Mixed Data', fontsize=25)\n",
    "\n",
    "axs[1].plot(recov_snms, lw=5)\n",
    "axs[1].set_title('Recovered Signal', fontsize=25)\n",
    "\n",
    "axs[2].plot(audio_files[1], lw=5)\n",
    "axs[2].set_title('Street Noise', fontsize=25)\n",
    "\n",
    "axs[3].plot(audio_files[2], lw=5)\n",
    "axs[3].set_title('Music Signal', fontsize=25)\n",
    "\n",
    "axs[4].plot(audio_files[0], lw=5)\n",
    "axs[4].set_title('Speech Signal', fontsize=25)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(0)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Time Fourier Transform (STFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street Noise + Music + Speec $d_{sn+m+s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, Zxx = signal.stft(d_sn_m_s, fs=sample_rates[0], nperseg=1000)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.pcolormesh(t, f, np.abs(Zxx))\n",
    "plt.title('STFT Magnitude')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylim(0, 7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music + White Noise $d_{m+wn}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, Zxx = signal.stft(d_m_wn, fs=sample_rates[0], nperseg=1000)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.pcolormesh(t, f, np.abs(Zxx))\n",
    "plt.title('STFT Magnitude')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylim(0, 7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech + White Noise $d_{s+wn}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, Zxx = signal.stft(d_s_wn, fs=sample_rates[0], nperseg=1000)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.pcolormesh(t, f, np.abs(Zxx))\n",
    "plt.title('STFT Magnitude')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylim(0, 7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street Noise + Music $d_{sn+m}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, Zxx = signal.stft(d_sn_m, fs=sample_rates[0], nperseg=1000)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.pcolormesh(t, f, np.abs(Zxx))\n",
    "plt.title('STFT Magnitude')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylim(0, 7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import padasip as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech + White Noise $d_{s+wn}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = len(audio_files[0])\n",
    "memory = 10\n",
    "u = audio_files[0]\n",
    "v = audio_files[3]\n",
    "d = d_s_wn\n",
    "\n",
    "x = pa.input_from_history(d, memory)[:-1]\n",
    "d = d[memory:]\n",
    "u = u[memory:]\n",
    "f = pa.filters.FilterRLS(mu=.5, n=memory)\n",
    "y, e, w = f.run(d, x)\n",
    "\n",
    "plt.figure(figsize=(12.5,6))\n",
    "plt.plot(u, \"r:\", linewidth=4, label=\"original\")\n",
    "plt.plot(d, \"b\", label=\"noisy\")\n",
    "plt.plot(y, \"g\", label=\"filtered\")\n",
    "plt.xlim(60000, 60100)\n",
    "plt.legend()\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlabel('Samples')\n",
    "plt.tight_layout()\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f290e9bb47e8cefb2a6a7867fb7ca058fb5b0f6b464d9af75d9c3f02494833d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('anc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
